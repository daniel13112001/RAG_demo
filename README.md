# RAG Financial Adviser Assistant

A simple Retrieval-Augmented Generation (RAG) system for financial advice with a web UI.

## ğŸ“‹ Project Structure

```
RAG_demo/
â”œâ”€â”€ ingest.py              # Build vector database from documents
â”œâ”€â”€ query.py               # Command-line query tool
â”œâ”€â”€ app.py                 # Flask server with REST API
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Web UI
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ style.css          # Styling
â”‚   â””â”€â”€ script.js          # Frontend logic
â”œâ”€â”€ data/                  # Your financial documents (PDF, DOCX, HTML, TXT, PPTX)
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ wealth_kb_index/   # FAISS vector database
â””â”€â”€ requirements.txt       # Python dependencies
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Set Up Environment Variables

Create a `.env` file in the project root with:

```
OPENAI_API_KEY=your-api-key-here
```

### 3. Ingest Documents (First Time Only)

Place your financial documents (PDF, DOCX, HTML, TXT, PPTX) in the `data/` folder, then:

```bash
python ingest.py
```

You should see output like:
```
ğŸ“„ Loading Client_Onboarding_Notes.txt ...
ğŸ“„ Loading Market_Risk_Education.html ...
âœ… Loaded 45 documents.
âœ‚ï¸  Split into 120 chunks.
ğŸ”¢ Generating embeddings...
ğŸ’¾ Creating FAISS index...
âœ… Vector DB saved to 'embeddings/wealth_kb_index'.
```

### 4. Start the Server

```bash
python app.py
```

You should see:
```
ğŸš€ Starting Flask server on http://127.0.0.1:5000
```

### 5. Open the Web UI

Open your browser and navigate to:
```
http://127.0.0.1:5000
```

## ğŸ’¬ How It Works

1. **User submits a question** via the web UI
2. **Backend retrieves relevant documents** from the FAISS vector database
3. **LLM generates an answer** using the retrieved context (using GPT-4o-mini)
4. **Response is displayed** with:
   - The LLM-generated answer
   - Sources used from your documents
   - Retrieved context chunks with relevance scores

## ğŸ¯ Features

- âœ… Simple web interface for querying
- âœ… RAG pipeline with context injection
- âœ… Support for multiple document types (PDF, DOCX, HTML, TXT, PPTX)
- âœ… Relevance scoring for retrieved documents
- âœ… Source attribution
- âœ… Built with Flask + LangChain + OpenAI

## ğŸ“ Configuration

Edit `app.py` or `query.py` to adjust:

- `CHUNK_SIZE`: Size of document chunks (default: 500)
- `CHUNK_OVERLAP`: Overlap between chunks (default: 100)
- `EMBEDDING_MODEL`: OpenAI embedding model (default: "text-embedding-3-small")
- `LLM_MODEL`: LLM to use (default: "gpt-4o-mini")
- `TOP_K`: Number of relevant chunks to retrieve (default: 3)

## ğŸ› ï¸ Troubleshooting

**"No FAISS index found"**
- Run `python ingest.py` first to build the vector database

**"Cannot connect to server"**
- Make sure `python app.py` is running
- Check that port 5000 is available

**API errors**
- Ensure your `OPENAI_API_KEY` is set in `.env`
- Check that you have sufficient OpenAI API credits

## ğŸ“š Next Steps

- Add more documents to `data/` and re-run `ingest.py`
- Customize the UI styling in `static/style.css`
- Modify system prompts in `app.py` for different use cases
- Deploy to cloud (Heroku, AWS, etc.)

---

**Created**: November 2025
**Tech Stack**: Flask, LangChain, OpenAI, FAISS, HTML/CSS/JavaScript
